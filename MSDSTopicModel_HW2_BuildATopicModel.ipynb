{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Topic-modeling-Amazon-Reviews/blob/master/MSDSTopicModel_HW2_BuildATopicModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWgYXQW1kuSg"
      },
      "source": [
        "# MSDS Marketing Text Analytics, Unit 2, Assignment 2: Build a topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚡️ Make a Copy\n",
        "\n",
        "Save a copy of this notebook in your Google Drive before continuing. Be sure to edit your own copy, not the original notebook."
      ],
      "metadata": {
        "id": "dLHoXywD4Cqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YbHDKkk1vq"
      },
      "source": [
        "In this assignment, you will implement a topic model preprocessor which can then be applied to the task of topic-modeling Amazon text reviews. Please review the course lectures and documentation up to this point before continuing. Be sure also to be familiar with the [documentation for TMToolkit](https://tmtoolkit.readthedocs.io/en/latest/topic_modeling.html)\n",
        "\n",
        "Be sure to make a copy into your own Drive account before editing this notebook.\n",
        "\n",
        "You will implement a preprocessing function to prepare your corpus for topic modeling. It is recommended that you use a small test corpus (an example is provided below) for development, rather than starting with the full review set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rfw11JBiZHi"
      },
      "source": [
        "## Imports\n",
        "\n",
        "**Note:** we have to do a good amount of dependency cleanup in order to get tmtoolkit working in Colab. You will likely see WARNINGS in the output below, but you should not see any ERRORs. In the end, you should end up with spacy 2.3.7, en_core_web_sm 2.3.1, and tmtoolkit 0.10.0.\n",
        "\n",
        "**Important:** You will also likely see a message to restart the runtime after the installations are complete, and should do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jpxYdJGQiWqj",
        "outputId": "f732ffe7-1ae6-4ce5-eb97-72457f035fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ecc4751cf8f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtmtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_lowercase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_clean_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtmtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTMPreproc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtmtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopicmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_ldamodel_topic_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtmtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopicmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtm_lda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_models_parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'lemmatize' from 'tmtoolkit.corpus' (/usr/local/lib/python3.7/dist-packages/tmtoolkit/corpus.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "try:\n",
        "    from tmtoolkit.corpus import Corpus\n",
        "    from tmtoolkit.preprocess import TMPreproc\n",
        "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
        "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --upgrade pip\n",
        "    !pip install lda\n",
        "    !pip install spacy-model-manager\n",
        "    !spacy-model remove en_core_web_sm\n",
        "    !pip uninstall -y spacy-model-manager\n",
        "    !pip uninstall -y spacy\n",
        "    !pip install spacy==2.3.7\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    !pip uninstall -y imgaug\n",
        "    !pip install \"imgaug<0.2.7,>=0.2.5\"\n",
        "    !pip install tmtoolkit==0.10.0\n",
        "    from tmtoolkit.corpus import Corpus\n",
        "    from tmtoolkit.preprocess import TMPreproc\n",
        "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
        "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKzLINMjD2n"
      },
      "source": [
        "**NOTE:** Loading a corpus as a list of strings is not the only way to use tmtoolkit. Given, for example, a large corpus that might not fit in memory, the current approach would not work well. See the tmtoolkit docs on [working with text corpora](https://tmtoolkit.readthedocs.io/en/latest/text_corpora.html) for more info."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8g9kSJwi7M7"
      },
      "source": [
        "## Implement a pre-processor\n",
        "\n",
        "Here you will implement a function called `preprocess` which returns the TMPreproc object to be used for topic modeling.\n",
        "\n",
        "The preprocess function will take a list of texts and return a pre-processed corpus object, i.e. a TMPreproc object. Preprocessing should include the following actions on the corpus using the appropriate methods in the TMPreproc class:\n",
        "\n",
        " - lemmatize the texts\n",
        " - convert tokens to lowercase\n",
        " - remove special characters\n",
        " - clean tokens to remove numbers and any tokens shorter than 3 characters\n",
        "\n",
        "The first part of the function to create the corpus and preprocess object are done for you. Your job is to call the specific preprocess functions and to return the resulting preprocess object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eQFevchKhohK"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts, lang=\"en\"):\n",
        "    \"\"\"Preprocessor which returns a TMPreproc object processed on corpus as language\n",
        "    specified by lang (defaults to \"en\"):\n",
        "\n",
        "    Should perform all of the following pre-processing functions:\n",
        "     - lemmatize\n",
        "     - tokens_to_lowercase\n",
        "     - remove_special_chars_in_tokens\n",
        "     - clean_tokens (remove numbers, and remove tokens shorter than 2)\n",
        "    \"\"\"\n",
        "    # Here, we just use the index of the text as the label for the corpus item\n",
        "    corpus = Corpus({ i:r for i, r in enumerate(texts) })\n",
        "    preproc = TMPreproc(corpus, language=lang).lemmatize().tokens_to_lowercase().remove_special_chars_in_tokens().clean_tokens(remove_shorter_than=2, remove_numbers=True)\n",
        "\n",
        "    # TODO: Complete the implementation of this function and submit the\n",
        "    # .py download of this notebook as your assignment submission.\n",
        "    return preproc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#~~ /autograde # do not delete this cell"
      ],
      "metadata": {
        "id": "r_jJg5-xRczD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2aoBerYnF2M"
      },
      "source": [
        "---\n",
        "### ⚠️  **Caution:** No arbitrary code above this line\n",
        "\n",
        "The only code written above should be the implementation of your graded function. For experimentation and testing, only add code below.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBlUJqEan3oc"
      },
      "source": [
        "## Function development\n",
        "\n",
        "Use this section of code to verify your function implementation. You may change the test_corpus as needed to verify your implementation. The grader will be checking that your function returns a TMPreproc object that meets all of the following critera:\n",
        "\n",
        " - tokens are lemmatized\n",
        " - tokens are converted to lowercase\n",
        " - special characters are removed from tokens\n",
        " - tokens shorter than 3 characters and numerics are removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YBlpS0RsrJ3s"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_tk2oDy5j1gQ",
        "outputId": "a75715fa-5bfa-451e-c053-39f9fe20c723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   0: {   'lemma': ['cat', 'sit', 'mat'],\n",
            "           'token': ['cat', 'sit', 'mat'],\n",
            "           'whitespace': [True, True, False]},\n",
            "    1: {   'lemma': ['fish', 'fish', 'Red', 'fish', 'Blue', 'fish'],\n",
            "           'token': ['fish', 'fish', 'red', 'fish', 'blue', 'fish'],\n",
            "           'whitespace': [True, True, True, True, True, False]},\n",
            "    2: {   'lemma': ['sell', 'ea$shells'],\n",
            "           'token': ['sell', 'eashells'],\n",
            "           'whitespace': [True, False]}}\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [ # Feel free to edit this corpus for further testing\n",
        "                # to be sure that your functions meet specifications.\n",
        "    \"The 3 cats sat on the mats!\",\n",
        "    \"1 fish 2 fish Red fish Blue fish\",\n",
        "    \"She sells $ea$shells\"\n",
        "]\n",
        "preproc = preprocess(test_corpus)\n",
        "pp.pprint(preproc.get_tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DuhlHcwQjs8T"
      },
      "outputs": [],
      "source": [
        "dtms = {\n",
        "    \"test_corpus\": preproc.dtm\n",
        "}\n",
        "lda_params = {\n",
        "    'n_topics': 2,\n",
        "    'eta': .01,\n",
        "    'n_iter': 10,\n",
        "    'random_state': 1234,  # to make results reproducible\n",
        "    'alpha': 1/16\n",
        "}\n",
        "\n",
        "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uvB8TL-muaM0",
        "outputId": "4fb7afde-cbd7-4e2b-99f7-f47d1071bc38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_1\n",
            "> #1. fish (0.566384)\n",
            "> #2. red (0.142655)\n",
            "> #3. cat (0.142655)\n",
            "> #4. blue (0.142655)\n",
            "> #5. sit (0.001412)\n",
            "topic_2\n",
            "> #1. sit (0.247549)\n",
            "> #2. sell (0.247549)\n",
            "> #3. mat (0.247549)\n",
            "> #4. eashells (0.247549)\n",
            "> #5. red (0.002451)\n"
          ]
        }
      ],
      "source": [
        "model = models[\"test_corpus\"][0][1]\n",
        "print_ldamodel_topic_words(model.topic_word_, preproc.vocabulary, top_n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8YqCs3kVFQ"
      },
      "source": [
        "### Assignment submission\n",
        "\n",
        "After completing the preprocess implementation, download your notebook as a .py file (File > Download > Download .py) and submit the downloaded file for grading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RmT0BazGQg"
      },
      "source": [
        "## Topic modeling Amazon Reviews\n",
        "\n",
        "Once you have completed the assignment above, you will be well prepared to start your final project for this unit. The project will include loading Amazon reviews into a corpus for topic modeling. The code below demonstrates topic modeling the reviews for a given brand. Note that the final project will require additional segmentation of the data, which is not done for you in the example here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V2YauVmrPav7",
        "outputId": "b4733046-9bb1-4064-9806-2bde2b61105f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RpcCkHX1P1d6"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "asins = []\n",
        "\n",
        "# To run this code, you will need to download the metadata file from the course\n",
        "# assets and upload it to your Google Drive. See the notes about that file\n",
        "# regarding how it was processed from the original file into json-l format.\n",
        "\n",
        "with gzip.open(\"drive/MyDrive/meta_Clothing_Shoes_and_Jewelry.jsonl.gz\") as products:\n",
        "    for product in products:\n",
        "        data = json.loads(product)\n",
        "        categories = [c.lower() for c in\n",
        "                      list(itertools.chain(*data.get(\"categories\", [])))]\n",
        "        if \"nike\" in categories:\n",
        "            asins.append(data[\"asin\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the first fews ASINs"
      ],
      "metadata": {
        "id": "KysOEFGwVfIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xycViEOjR4Gr",
        "outputId": "4f2bf7bf-7454-4ded-f388-c18b155911a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B0000V9K32', 'B0000V9K3W', 'B0000V9K46']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "asins[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the length, i.e. the number of resulting ASINs"
      ],
      "metadata": {
        "id": "P0Gska4bVj3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AIqkMoAvU-Sz",
        "outputId": "04ea5fc0-d5e6-4374-ac24-1d8ba5d585e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8327"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(asins)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a corpus of review texts"
      ],
      "metadata": {
        "id": "y0bwBkHsVrsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u6HT0Yjy3gNG"
      },
      "outputs": [],
      "source": [
        "review_corpus = []\n",
        "with gzip.open(\"drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\") as reviews:\n",
        "    for review in reviews:\n",
        "        data = json.loads(review)\n",
        "        if data[\"asin\"] in asins:\n",
        "            text = data[\"reviewText\"]\n",
        "            review_corpus.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect a few of the reviews"
      ],
      "metadata": {
        "id": "hKrH89AKVwCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A9Ct4cj7Tih9",
        "outputId": "34d58663-dcb7-41eb-fa47-7b57540a80cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 the colour i received is not blue as shown but yellow.Couldnt change it because \n",
            "1 Very cute and is really practical. Fits better on smaller wrists which is my cas\n",
            "2 The watch was exactly what i ordered and I got it very fast. Unfortunately it wa\n",
            "3 This product came promptly and as described, pleasure doing business with them!-\n",
            "4 Why isn't Nike making these anymore?  I love this watch, and I get a lot of comp\n"
          ]
        }
      ],
      "source": [
        "for i, review in enumerate(review_corpus[:5]):\n",
        "    print(i, review[:80])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a TMPreproc object from the review corpus"
      ],
      "metadata": {
        "id": "q9ppd90QVzgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "taIJ_BZU7E81"
      },
      "outputs": [],
      "source": [
        "pre = preprocess(review_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wq_-wpEE8cnV",
        "outputId": "06e34bdc-e9bb-47cd-9b27-eafd91557ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lda:all zero row in document-term matrix found\n"
          ]
        }
      ],
      "source": [
        "dtms = {\n",
        "    \"reviews_corpus\": pre.dtm\n",
        "}\n",
        "lda_params = {\n",
        "    'n_topics': 10,\n",
        "    'eta': .01,\n",
        "    'n_iter': 10,\n",
        "    'random_state': 1234,  # to make results reproducible\n",
        "    'alpha': 1/16\n",
        "}\n",
        "\n",
        "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the topics"
      ],
      "metadata": {
        "id": "RGRrYqRuV7nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[\"reviews_corpus\"][0][1]\n",
        "print_ldamodel_topic_words(model.topic_word_, pre.vocabulary, top_n=5)"
      ],
      "metadata": {
        "id": "D8GYqMPHjVH7",
        "outputId": "7f77abc0-6866-41ec-eb73-2e52c2cfdd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_1\n",
            "> #1. shoe (0.046912)\n",
            "> #2. good (0.023715)\n",
            "> #3. fit (0.021885)\n",
            "> #4. size (0.018012)\n",
            "> #5. comfortable (0.017603)\n",
            "topic_2\n",
            "> #1. shoe (0.029601)\n",
            "> #2. nike (0.015272)\n",
            "> #3. watch (0.014036)\n",
            "> #4. great (0.012632)\n",
            "> #5. look (0.012486)\n",
            "topic_3\n",
            "> #1. shoe (0.030574)\n",
            "> #2. size (0.019319)\n",
            "> #3. fit (0.012594)\n",
            "> #4. buy (0.011312)\n",
            "> #5. love (0.011284)\n",
            "topic_4\n",
            "> #1. shoe (0.035784)\n",
            "> #2. watch (0.018186)\n",
            "> #3. good (0.017141)\n",
            "> #4. great (0.016400)\n",
            "> #5. nike (0.012763)\n",
            "topic_5\n",
            "> #1. shoe (0.038853)\n",
            "> #2. great (0.017597)\n",
            "> #3. wear (0.014864)\n",
            "> #4. get (0.014170)\n",
            "> #5. love (0.013813)\n",
            "topic_6\n",
            "> #1. shoe (0.047699)\n",
            "> #2. great (0.017569)\n",
            "> #3. run (0.016964)\n",
            "> #4. love (0.015816)\n",
            "> #5. fit (0.014231)\n",
            "topic_7\n",
            "> #1. shoe (0.033067)\n",
            "> #2. wear (0.015657)\n",
            "> #3. buy (0.013333)\n",
            "> #4. good (0.012803)\n",
            "> #5. foot (0.011926)\n",
            "topic_8\n",
            "> #1. shoe (0.049415)\n",
            "> #2. good (0.016635)\n",
            "> #3. great (0.016451)\n",
            "> #4. wear (0.016002)\n",
            "> #5. love (0.015982)\n",
            "topic_9\n",
            "> #1. shoe (0.055937)\n",
            "> #2. fit (0.019548)\n",
            "> #3. run (0.018157)\n",
            "> #4. love (0.017527)\n",
            "> #5. great (0.016210)\n",
            "topic_10\n",
            "> #1. shoe (0.029931)\n",
            "> #2. good (0.018878)\n",
            "> #3. great (0.014214)\n",
            "> #4. comfortable (0.011252)\n",
            "> #5. like (0.011053)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save your topic model and corpus for use in Lab 2"
      ],
      "metadata": {
        "id": "Z7Mci4DlcB64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have completed the above assignment, run the following code to save your topic model and your corpus to your Google Drive. You will load this model and use it for document classification in Lab 2."
      ],
      "metadata": {
        "id": "uk6JySzqcGIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
        "\n",
        "with open(\"drive/MyDrive/MSDS_HW2_model.p\", \"wb\") as modelfile:\n",
        "    save_ldamodel_to_pickle(modelfile, model, pre.vocabulary, pre.doc_labels, dtm=pre.dtm)"
      ],
      "metadata": {
        "id": "qVXbd0AUcJIr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/MSDS_HW2_corpus.p\", \"wb\") as corpusfile:\n",
        "    pickle.dump(review_corpus, corpusfile)"
      ],
      "metadata": {
        "id": "PVhycwCNcL4w"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}